{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmKbqjTl/8uYFdWiGtlwDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safdarjung/saf/blob/main/Pytorch_MNIST_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJmvyPt5jiQF",
        "outputId": "97af2507-1cf1-4bcb-d719-28521e7520c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "mnist.keys()\n",
        "\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are your data and target variables respectively\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GMWVUD9akK1e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "7XwzlLUikR2D",
        "outputId": "52613b1b-91cd-46f0-cf3d-9a6e7cbe5993"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
              "0           0       0       0       0       0       0       0       0       0   \n",
              "1           0       0       0       0       0       0       0       0       0   \n",
              "2           0       0       0       0       0       0       0       0       0   \n",
              "3           0       0       0       0       0       0       0       0       0   \n",
              "4           0       0       0       0       0       0       0       0       0   \n",
              "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "69995       0       0       0       0       0       0       0       0       0   \n",
              "69996       0       0       0       0       0       0       0       0       0   \n",
              "69997       0       0       0       0       0       0       0       0       0   \n",
              "69998       0       0       0       0       0       0       0       0       0   \n",
              "69999       0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0            0  ...         0         0         0         0         0   \n",
              "1            0  ...         0         0         0         0         0   \n",
              "2            0  ...         0         0         0         0         0   \n",
              "3            0  ...         0         0         0         0         0   \n",
              "4            0  ...         0         0         0         0         0   \n",
              "...        ...  ...       ...       ...       ...       ...       ...   \n",
              "69995        0  ...         0         0         0         0         0   \n",
              "69996        0  ...         0         0         0         0         0   \n",
              "69997        0  ...         0         0         0         0         0   \n",
              "69998        0  ...         0         0         0         0         0   \n",
              "69999        0  ...         0         0         0         0         0   \n",
              "\n",
              "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
              "0             0         0         0         0         0  \n",
              "1             0         0         0         0         0  \n",
              "2             0         0         0         0         0  \n",
              "3             0         0         0         0         0  \n",
              "4             0         0         0         0         0  \n",
              "...         ...       ...       ...       ...       ...  \n",
              "69995         0         0         0         0         0  \n",
              "69996         0         0         0         0         0  \n",
              "69997         0         0         0         0         0  \n",
              "69998         0         0         0         0         0  \n",
              "69999         0         0         0         0         0  \n",
              "\n",
              "[70000 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c35bca3-f018-4c24-b181-72cbabd956a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 784 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c35bca3-f018-4c24-b181-72cbabd956a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c35bca3-f018-4c24-b181-72cbabd956a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c35bca3-f018-4c24-b181-72cbabd956a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-864364bd-1ef6-4d5b-8a5b-7a344ac978aa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-864364bd-1ef6-4d5b-8a5b-7a344ac978aa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-864364bd-1ef6-4d5b-8a5b-7a344ac978aa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cd788119-8533-4652-b67e-669cf30956ef\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cd788119-8533-4652-b67e-669cf30956ef button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "PY18SZo6kVc6",
        "outputId": "e8a3c79f-58c1-48c2-eb34-f8b1e7279c98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47339    5\n",
              "67456    4\n",
              "12308    8\n",
              "32557    0\n",
              "664      2\n",
              "        ..\n",
              "37194    6\n",
              "6265     6\n",
              "54886    1\n",
              "860      0\n",
              "15795    0\n",
              "Name: class, Length: 56000, dtype: category\n",
              "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47339</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67456</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12308</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37194</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6265</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54886</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> category</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "i8iNBDbXkahN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SKG7pX1Xk0Vh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, X,y, transform=None, is_test=False):\n",
        "    self.dataframe = pd.DataFrame(X)\n",
        "    self.y_vals = pd.DataFrame(y)\n",
        "    self.transform = transform\n",
        "    self.is_test = is_test\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataframe.iloc[idx]\n",
        "    y_vals = self.y_vals.iloc[idx]\n",
        "    if self.is_test:\n",
        "      image = item.values.reshape(28, 28).astype(np.uint8)\n",
        "      label = None\n",
        "    else:\n",
        "      image = item.values.reshape(28, 28).astype(np.uint8)\n",
        "      label = y_vals.iloc[0]\n",
        "    image = transforms.ToPILImage()(image)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if self.is_test:\n",
        "      return image\n",
        "    else:\n",
        "      return image, label\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "1ryofiFHk8WR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "q7Qe7RDlmrTl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train,y_train, transform=transform, is_test=False)\n",
        "test_dataset = CustomDataset(X_test, y_test,transform=transform, is_test=True)"
      ],
      "metadata": {
        "id": "P3aX6xrGnATa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiwzRRAfnVW8",
        "outputId": "f8ec5d37-a72a-42cf-e4e7-729a742324e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56000\n",
            "14000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IcjwUCwocUW",
        "outputId": "9bf38fa4-5149-4bf4-dc17-a50531e2e80b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000,  0.5529,  0.5529, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -0.8039,  0.7333, -0.7882, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9608, -0.0667,  0.4667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333, -0.7490,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.1373,  0.9922, -0.7020, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,  0.9529,  0.0902,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1922,\n",
              "            0.9137,  0.8039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000,  0.0980,  0.9843,  0.0118, -0.9765,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.8980,\n",
              "            0.7804, -0.6471, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000,  0.9529,  0.9294, -0.4275, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,  0.8824,  0.9922,\n",
              "           -0.3176, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -0.1765,  0.9294, -0.4118, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8667,  0.8275,  0.9922,\n",
              "            0.0039, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -0.7490,  0.9765,  0.2078, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -0.9451,  0.5216,  0.9922,  0.5294,\n",
              "           -0.8824, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -0.1216,  1.0000, -0.7255, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000,  0.0039,  0.9922,  0.8902, -0.7176,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -0.8039,  0.9059,  0.9608, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -0.8745,  0.9922,  0.9922, -0.4824, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000,  0.0039,  0.9922,  0.2941, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -0.8745, -0.0196,  0.9922,  0.1373, -0.9686, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.7176,  0.8824,  0.9922, -0.9294, -1.0000, -1.0000, -0.4431,\n",
              "            0.4980,  0.7412,  0.9922,  0.1373, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.5059,  0.9922,  0.9922, -0.9686, -0.9373,  0.1451,  0.9765,\n",
              "            0.9922,  0.9922,  0.9922, -0.9686, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.5059,  0.9922,  0.9922, -0.9686,  0.8510,  0.8667,  0.9922,\n",
              "            0.9922,  0.9922,  0.9922,  0.5137, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -0.7725,  0.7255,  0.9922,  0.9216,  0.8510, -0.3725,\n",
              "            0.6549,  0.9922,  0.4824, -0.8196, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -0.3333,  0.9216, -1.0000, -1.0000, -0.0431,\n",
              "            0.9765,  0.6784, -0.5373, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7098,  0.9451,\n",
              "            0.8196, -0.3569, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.5373,  0.9843,\n",
              "           -0.4745, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -0.3412,  0.9922, -0.1686,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000,  0.8824,  0.7961, -0.7255,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9686, -0.5137,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n",
              " '4')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "en39zaDiopOP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex, label in train_loader:\n",
        "  img = ex[0]\n",
        "  print(ex.size())\n",
        "  img_np = img.permute(1,2,0).numpy()\n",
        "  plt.imshow(img_np, cmap='gray')\n",
        "  plt.title(f\"Label: {label[0]}\")\n",
        "  plt.show()\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Kg-hb6aMq6ZZ",
        "outputId": "c3620ca0-c680-4a85-8af2-be5dfcbc5c36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# ((Width-Kernel + 2*padding)/Stride) + 1\n",
        "\n",
        "    self.fc1 = nn.Linear(128*7*7,128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.fc2 = nn.Linear(128,20)\n",
        "    self.fc3 = nn.Linear(20,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x_size = x.size(1) * x.size(2) * x.size(3)\n",
        "\n",
        "    x = x.view(-1, x_size)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "YizMne5irM0_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "HUH1B8g_u2Ux"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcFfn4xPwBYw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 150\n",
        "running_loss = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # Unpack the data correctly\n",
        "        inputs, labels = data[0], data[1]  # First element is inputs, second is labels\n",
        "\n",
        "        # Convert labels to integers if they are strings\n",
        "        labels = torch.tensor([int(label) for label in labels]).to(device)\n",
        "\n",
        "        # Move tensors to device\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Br6Se9YSvLxk",
        "outputId": "046ec87c-7109-47ff-f08f-2421a143add2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.300596079826355\n",
            "Epoch 1, Batch 200, Loss: 2.292385959625244\n",
            "Epoch 1, Batch 300, Loss: 2.28048321723938\n",
            "Epoch 1, Batch 400, Loss: 2.2590389823913575\n",
            "Epoch 1, Batch 500, Loss: 2.208803696632385\n",
            "Epoch 1, Batch 600, Loss: 2.0784059512615203\n",
            "Epoch 1, Batch 700, Loss: 1.6907225859165191\n",
            "Epoch 1, Batch 800, Loss: 1.077574976682663\n",
            "Epoch 2, Batch 100, Loss: 1.2702546003460884\n",
            "Epoch 2, Batch 200, Loss: 0.5785110965371132\n",
            "Epoch 2, Batch 300, Loss: 0.5279380878806115\n",
            "Epoch 2, Batch 400, Loss: 0.4556771919131279\n",
            "Epoch 2, Batch 500, Loss: 0.4034952554106712\n",
            "Epoch 2, Batch 600, Loss: 0.4121650032699108\n",
            "Epoch 2, Batch 700, Loss: 0.3536486104130745\n",
            "Epoch 2, Batch 800, Loss: 0.35886589869856833\n",
            "Epoch 3, Batch 100, Loss: 0.5541830371320248\n",
            "Epoch 3, Batch 200, Loss: 0.2974220025539398\n",
            "Epoch 3, Batch 300, Loss: 0.29506283454597\n",
            "Epoch 3, Batch 400, Loss: 0.272386604771018\n",
            "Epoch 3, Batch 500, Loss: 0.26837688460946085\n",
            "Epoch 3, Batch 600, Loss: 0.2601052873581648\n",
            "Epoch 3, Batch 700, Loss: 0.23684518091380596\n",
            "Epoch 3, Batch 800, Loss: 0.22602062478661536\n",
            "Epoch 4, Batch 100, Loss: 0.37688703905791043\n",
            "Epoch 4, Batch 200, Loss: 0.21328055813908578\n",
            "Epoch 4, Batch 300, Loss: 0.1947593403980136\n",
            "Epoch 4, Batch 400, Loss: 0.2089699547737837\n",
            "Epoch 4, Batch 500, Loss: 0.18625556487590075\n",
            "Epoch 4, Batch 600, Loss: 0.19002093750983476\n",
            "Epoch 4, Batch 700, Loss: 0.17234878279268742\n",
            "Epoch 4, Batch 800, Loss: 0.16205588150769473\n",
            "Epoch 5, Batch 100, Loss: 0.2803444561734796\n",
            "Epoch 5, Batch 200, Loss: 0.15692737091332673\n",
            "Epoch 5, Batch 300, Loss: 0.1603671745955944\n",
            "Epoch 5, Batch 400, Loss: 0.1595700415223837\n",
            "Epoch 5, Batch 500, Loss: 0.1583113120868802\n",
            "Epoch 5, Batch 600, Loss: 0.1351224454306066\n",
            "Epoch 5, Batch 700, Loss: 0.13978246867656707\n",
            "Epoch 5, Batch 800, Loss: 0.149460517950356\n",
            "Epoch 6, Batch 100, Loss: 0.23317932438105346\n",
            "Epoch 6, Batch 200, Loss: 0.13974236549809574\n",
            "Epoch 6, Batch 300, Loss: 0.12758738355711102\n",
            "Epoch 6, Batch 400, Loss: 0.1255303572304547\n",
            "Epoch 6, Batch 500, Loss: 0.1383570337295532\n",
            "Epoch 6, Batch 600, Loss: 0.13173823541030288\n",
            "Epoch 6, Batch 700, Loss: 0.12976781513541938\n",
            "Epoch 6, Batch 800, Loss: 0.1186863598972559\n",
            "Epoch 7, Batch 100, Loss: 0.20692652761936187\n",
            "Epoch 7, Batch 200, Loss: 0.11523116765543819\n",
            "Epoch 7, Batch 300, Loss: 0.10450409054756164\n",
            "Epoch 7, Batch 400, Loss: 0.1124889287725091\n",
            "Epoch 7, Batch 500, Loss: 0.11228744488209486\n",
            "Epoch 7, Batch 600, Loss: 0.10583761627785862\n",
            "Epoch 7, Batch 700, Loss: 0.11281652571167797\n",
            "Epoch 7, Batch 800, Loss: 0.11415045455098152\n",
            "Epoch 8, Batch 100, Loss: 0.1741378824133426\n",
            "Epoch 8, Batch 200, Loss: 0.09727729625999927\n",
            "Epoch 8, Batch 300, Loss: 0.10723342325538397\n",
            "Epoch 8, Batch 400, Loss: 0.09760110505856573\n",
            "Epoch 8, Batch 500, Loss: 0.10644848191179335\n",
            "Epoch 8, Batch 600, Loss: 0.10677219243720174\n",
            "Epoch 8, Batch 700, Loss: 0.10826876850798726\n",
            "Epoch 8, Batch 800, Loss: 0.10127881839871407\n",
            "Epoch 9, Batch 100, Loss: 0.17454230126924813\n",
            "Epoch 9, Batch 200, Loss: 0.09572970212437212\n",
            "Epoch 9, Batch 300, Loss: 0.09491947147995233\n",
            "Epoch 9, Batch 400, Loss: 0.10197115505114197\n",
            "Epoch 9, Batch 500, Loss: 0.08871837612241507\n",
            "Epoch 9, Batch 600, Loss: 0.09868425093591213\n",
            "Epoch 9, Batch 700, Loss: 0.09225437886081636\n",
            "Epoch 9, Batch 800, Loss: 0.10732378153130412\n",
            "Epoch 10, Batch 100, Loss: 0.15582778368145228\n",
            "Epoch 10, Batch 200, Loss: 0.09286003292538225\n",
            "Epoch 10, Batch 300, Loss: 0.09038594569079578\n",
            "Epoch 10, Batch 400, Loss: 0.0812902844697237\n",
            "Epoch 10, Batch 500, Loss: 0.08524658393114805\n",
            "Epoch 10, Batch 600, Loss: 0.08123767517041415\n",
            "Epoch 10, Batch 700, Loss: 0.08220342941116542\n",
            "Epoch 10, Batch 800, Loss: 0.07883838173002004\n",
            "Epoch 11, Batch 100, Loss: 0.14964874268043787\n",
            "Epoch 11, Batch 200, Loss: 0.07724230684805661\n",
            "Epoch 11, Batch 300, Loss: 0.08787630430422723\n",
            "Epoch 11, Batch 400, Loss: 0.09008726376108825\n",
            "Epoch 11, Batch 500, Loss: 0.08418895076029002\n",
            "Epoch 11, Batch 600, Loss: 0.07400632578879594\n",
            "Epoch 11, Batch 700, Loss: 0.0790221187286079\n",
            "Epoch 11, Batch 800, Loss: 0.07971902026329189\n",
            "Epoch 12, Batch 100, Loss: 0.13252187715377658\n",
            "Epoch 12, Batch 200, Loss: 0.08035538958385587\n",
            "Epoch 12, Batch 300, Loss: 0.09083317333832383\n",
            "Epoch 12, Batch 400, Loss: 0.07493086808361113\n",
            "Epoch 12, Batch 500, Loss: 0.08018976783845573\n",
            "Epoch 12, Batch 600, Loss: 0.07634555508382618\n",
            "Epoch 12, Batch 700, Loss: 0.08072002747096121\n",
            "Epoch 12, Batch 800, Loss: 0.08214237953536213\n",
            "Epoch 13, Batch 100, Loss: 0.13339830024167895\n",
            "Epoch 13, Batch 200, Loss: 0.07224697302561253\n",
            "Epoch 13, Batch 300, Loss: 0.07945386188104749\n",
            "Epoch 13, Batch 400, Loss: 0.07598834913689644\n",
            "Epoch 13, Batch 500, Loss: 0.07545657344628126\n",
            "Epoch 13, Batch 600, Loss: 0.07051495787687599\n",
            "Epoch 13, Batch 700, Loss: 0.07613384938333184\n",
            "Epoch 13, Batch 800, Loss: 0.07641659755492583\n",
            "Epoch 14, Batch 100, Loss: 0.13329367871396244\n",
            "Epoch 14, Batch 200, Loss: 0.07612843005917966\n",
            "Epoch 14, Batch 300, Loss: 0.061213326030410825\n",
            "Epoch 14, Batch 400, Loss: 0.0661261512991041\n",
            "Epoch 14, Batch 500, Loss: 0.06796897917054594\n",
            "Epoch 14, Batch 600, Loss: 0.07187995534390211\n",
            "Epoch 14, Batch 700, Loss: 0.0718019855581224\n",
            "Epoch 14, Batch 800, Loss: 0.07033348933793604\n",
            "Epoch 15, Batch 100, Loss: 0.12700673035345972\n",
            "Epoch 15, Batch 200, Loss: 0.06730276757851243\n",
            "Epoch 15, Batch 300, Loss: 0.0696921502519399\n",
            "Epoch 15, Batch 400, Loss: 0.06135007738601416\n",
            "Epoch 15, Batch 500, Loss: 0.07674689350184054\n",
            "Epoch 15, Batch 600, Loss: 0.06431310860905796\n",
            "Epoch 15, Batch 700, Loss: 0.07495311647187919\n",
            "Epoch 15, Batch 800, Loss: 0.06149712326005101\n",
            "Epoch 16, Batch 100, Loss: 0.11346462132409214\n",
            "Epoch 16, Batch 200, Loss: 0.058235636865720156\n",
            "Epoch 16, Batch 300, Loss: 0.06789120468776673\n",
            "Epoch 16, Batch 400, Loss: 0.06523319630883634\n",
            "Epoch 16, Batch 500, Loss: 0.0709452449483797\n",
            "Epoch 16, Batch 600, Loss: 0.06880944061093032\n",
            "Epoch 16, Batch 700, Loss: 0.06677242153324187\n",
            "Epoch 16, Batch 800, Loss: 0.06513840378727764\n",
            "Epoch 17, Batch 100, Loss: 0.11466084633488208\n",
            "Epoch 17, Batch 200, Loss: 0.05442912617232651\n",
            "Epoch 17, Batch 300, Loss: 0.060904915211722255\n",
            "Epoch 17, Batch 400, Loss: 0.05714124039979652\n",
            "Epoch 17, Batch 500, Loss: 0.06996110662817955\n",
            "Epoch 17, Batch 600, Loss: 0.06930557614192366\n",
            "Epoch 17, Batch 700, Loss: 0.06358093501068651\n",
            "Epoch 17, Batch 800, Loss: 0.05334432472474873\n",
            "Epoch 18, Batch 100, Loss: 0.10076310123782604\n",
            "Epoch 18, Batch 200, Loss: 0.060375101240351794\n",
            "Epoch 18, Batch 300, Loss: 0.06297117949929089\n",
            "Epoch 18, Batch 400, Loss: 0.05761825837660581\n",
            "Epoch 18, Batch 500, Loss: 0.0688916976749897\n",
            "Epoch 18, Batch 600, Loss: 0.05834070067387074\n",
            "Epoch 18, Batch 700, Loss: 0.0669402459822595\n",
            "Epoch 18, Batch 800, Loss: 0.06032189157092944\n",
            "Epoch 19, Batch 100, Loss: 0.10855900809168816\n",
            "Epoch 19, Batch 200, Loss: 0.05239851938327775\n",
            "Epoch 19, Batch 300, Loss: 0.06798851370578632\n",
            "Epoch 19, Batch 400, Loss: 0.051127580413594845\n",
            "Epoch 19, Batch 500, Loss: 0.05934601383516565\n",
            "Epoch 19, Batch 600, Loss: 0.058426165969576685\n",
            "Epoch 19, Batch 700, Loss: 0.059765336241107435\n",
            "Epoch 19, Batch 800, Loss: 0.05618172605056316\n",
            "Epoch 20, Batch 100, Loss: 0.09747528509935365\n",
            "Epoch 20, Batch 200, Loss: 0.05703662390355021\n",
            "Epoch 20, Batch 300, Loss: 0.057879735715687275\n",
            "Epoch 20, Batch 400, Loss: 0.058990821936167774\n",
            "Epoch 20, Batch 500, Loss: 0.06309486695565283\n",
            "Epoch 20, Batch 600, Loss: 0.0655993985850364\n",
            "Epoch 20, Batch 700, Loss: 0.05738683499628678\n",
            "Epoch 20, Batch 800, Loss: 0.05835547802504152\n",
            "Epoch 21, Batch 100, Loss: 0.10757346088066697\n",
            "Epoch 21, Batch 200, Loss: 0.05781472347676754\n",
            "Epoch 21, Batch 300, Loss: 0.052979542738758025\n",
            "Epoch 21, Batch 400, Loss: 0.0624976014601998\n",
            "Epoch 21, Batch 500, Loss: 0.04805142415687442\n",
            "Epoch 21, Batch 600, Loss: 0.0638350980100222\n",
            "Epoch 21, Batch 700, Loss: 0.05367146966280416\n",
            "Epoch 21, Batch 800, Loss: 0.054057337290141734\n",
            "Epoch 22, Batch 100, Loss: 0.08643624119693413\n",
            "Epoch 22, Batch 200, Loss: 0.04732627019518986\n",
            "Epoch 22, Batch 300, Loss: 0.05441463841823861\n",
            "Epoch 22, Batch 400, Loss: 0.057589892798569055\n",
            "Epoch 22, Batch 500, Loss: 0.04972447969252244\n",
            "Epoch 22, Batch 600, Loss: 0.06120408135466278\n",
            "Epoch 22, Batch 700, Loss: 0.06823491073213518\n",
            "Epoch 22, Batch 800, Loss: 0.057244561617262664\n",
            "Epoch 23, Batch 100, Loss: 0.0873632727516815\n",
            "Epoch 23, Batch 200, Loss: 0.05434475878952071\n",
            "Epoch 23, Batch 300, Loss: 0.05110386824468151\n",
            "Epoch 23, Batch 400, Loss: 0.04829082921380177\n",
            "Epoch 23, Batch 500, Loss: 0.05366753903683275\n",
            "Epoch 23, Batch 600, Loss: 0.052314401641488074\n",
            "Epoch 23, Batch 700, Loss: 0.05828583936672658\n",
            "Epoch 23, Batch 800, Loss: 0.05478321666363627\n",
            "Epoch 24, Batch 100, Loss: 0.0936378920567222\n",
            "Epoch 24, Batch 200, Loss: 0.04675614149775356\n",
            "Epoch 24, Batch 300, Loss: 0.050548491030931474\n",
            "Epoch 24, Batch 400, Loss: 0.04969419670058414\n",
            "Epoch 24, Batch 500, Loss: 0.053731404119171204\n",
            "Epoch 24, Batch 600, Loss: 0.044711428480222824\n",
            "Epoch 24, Batch 700, Loss: 0.053698651501908896\n",
            "Epoch 24, Batch 800, Loss: 0.05225267091882415\n",
            "Epoch 25, Batch 100, Loss: 0.09434919384075328\n",
            "Epoch 25, Batch 200, Loss: 0.05591567278141156\n",
            "Epoch 25, Batch 300, Loss: 0.05570075293537229\n",
            "Epoch 25, Batch 400, Loss: 0.04915345514891669\n",
            "Epoch 25, Batch 500, Loss: 0.048298764976207165\n",
            "Epoch 25, Batch 600, Loss: 0.053331316609401255\n",
            "Epoch 25, Batch 700, Loss: 0.0566889626160264\n",
            "Epoch 25, Batch 800, Loss: 0.05252268514304888\n",
            "Epoch 26, Batch 100, Loss: 0.09846667686593719\n",
            "Epoch 26, Batch 200, Loss: 0.05106125845108181\n",
            "Epoch 26, Batch 300, Loss: 0.04345431714784354\n",
            "Epoch 26, Batch 400, Loss: 0.04470420376397669\n",
            "Epoch 26, Batch 500, Loss: 0.04481931378599256\n",
            "Epoch 26, Batch 600, Loss: 0.0472778357192874\n",
            "Epoch 26, Batch 700, Loss: 0.051370427338406445\n",
            "Epoch 26, Batch 800, Loss: 0.05577159516746178\n",
            "Epoch 27, Batch 100, Loss: 0.0804006375244353\n",
            "Epoch 27, Batch 200, Loss: 0.04520582915050909\n",
            "Epoch 27, Batch 300, Loss: 0.04438045049086213\n",
            "Epoch 27, Batch 400, Loss: 0.04504659092053771\n",
            "Epoch 27, Batch 500, Loss: 0.04540193017339334\n",
            "Epoch 27, Batch 600, Loss: 0.05293762317625806\n",
            "Epoch 27, Batch 700, Loss: 0.045851337295025586\n",
            "Epoch 27, Batch 800, Loss: 0.0524440879386384\n",
            "Epoch 28, Batch 100, Loss: 0.0739953371451702\n",
            "Epoch 28, Batch 200, Loss: 0.042302506007254125\n",
            "Epoch 28, Batch 300, Loss: 0.043875091031659395\n",
            "Epoch 28, Batch 400, Loss: 0.04310893480316736\n",
            "Epoch 28, Batch 500, Loss: 0.05120254744077101\n",
            "Epoch 28, Batch 600, Loss: 0.04740089217433706\n",
            "Epoch 28, Batch 700, Loss: 0.03977520470507443\n",
            "Epoch 28, Batch 800, Loss: 0.0557520285755163\n",
            "Epoch 29, Batch 100, Loss: 0.07317232120549307\n",
            "Epoch 29, Batch 200, Loss: 0.03948860220611095\n",
            "Epoch 29, Batch 300, Loss: 0.04842570962617174\n",
            "Epoch 29, Batch 400, Loss: 0.035300245326943695\n",
            "Epoch 29, Batch 500, Loss: 0.04175299836555496\n",
            "Epoch 29, Batch 600, Loss: 0.05235463933786377\n",
            "Epoch 29, Batch 700, Loss: 0.04534900309983641\n",
            "Epoch 29, Batch 800, Loss: 0.04287420990527607\n",
            "Epoch 30, Batch 100, Loss: 0.0717599604697898\n",
            "Epoch 30, Batch 200, Loss: 0.04114106599707156\n",
            "Epoch 30, Batch 300, Loss: 0.045541901449905706\n",
            "Epoch 30, Batch 400, Loss: 0.04171293256455101\n",
            "Epoch 30, Batch 500, Loss: 0.03996088236570358\n",
            "Epoch 30, Batch 600, Loss: 0.043504234614083544\n",
            "Epoch 30, Batch 700, Loss: 0.045289797529112545\n",
            "Epoch 30, Batch 800, Loss: 0.04471371890278533\n",
            "Epoch 31, Batch 100, Loss: 0.0806197627261281\n",
            "Epoch 31, Batch 200, Loss: 0.04655016780132428\n",
            "Epoch 31, Batch 300, Loss: 0.048646016032435\n",
            "Epoch 31, Batch 400, Loss: 0.04173697465565056\n",
            "Epoch 31, Batch 500, Loss: 0.04497338427696377\n",
            "Epoch 31, Batch 600, Loss: 0.04266511996742338\n",
            "Epoch 31, Batch 700, Loss: 0.038750011795200406\n",
            "Epoch 31, Batch 800, Loss: 0.04830317151616328\n",
            "Epoch 32, Batch 100, Loss: 0.07438937401282601\n",
            "Epoch 32, Batch 200, Loss: 0.03224753197631799\n",
            "Epoch 32, Batch 300, Loss: 0.03600794687634334\n",
            "Epoch 32, Batch 400, Loss: 0.0425401563802734\n",
            "Epoch 32, Batch 500, Loss: 0.03403573234099895\n",
            "Epoch 32, Batch 600, Loss: 0.03903223066590726\n",
            "Epoch 32, Batch 700, Loss: 0.04375452812761069\n",
            "Epoch 32, Batch 800, Loss: 0.04846901710436214\n",
            "Epoch 33, Batch 100, Loss: 0.07107477096142248\n",
            "Epoch 33, Batch 200, Loss: 0.0354657265660353\n",
            "Epoch 33, Batch 300, Loss: 0.03691619834746234\n",
            "Epoch 33, Batch 400, Loss: 0.040145263336598874\n",
            "Epoch 33, Batch 500, Loss: 0.03676966464845464\n",
            "Epoch 33, Batch 600, Loss: 0.037129546746145936\n",
            "Epoch 33, Batch 700, Loss: 0.043869498362764714\n",
            "Epoch 33, Batch 800, Loss: 0.04605796930612996\n",
            "Epoch 34, Batch 100, Loss: 0.076096662490163\n",
            "Epoch 34, Batch 200, Loss: 0.040646501872688534\n",
            "Epoch 34, Batch 300, Loss: 0.039684769043233245\n",
            "Epoch 34, Batch 400, Loss: 0.042862542467191816\n",
            "Epoch 34, Batch 500, Loss: 0.040742303081788125\n",
            "Epoch 34, Batch 600, Loss: 0.04744682154385373\n",
            "Epoch 34, Batch 700, Loss: 0.038756173248402775\n",
            "Epoch 34, Batch 800, Loss: 0.0416698602726683\n",
            "Epoch 35, Batch 100, Loss: 0.07560442309186328\n",
            "Epoch 35, Batch 200, Loss: 0.03901875967625529\n",
            "Epoch 35, Batch 300, Loss: 0.03712881283601746\n",
            "Epoch 35, Batch 400, Loss: 0.043171205971739256\n",
            "Epoch 35, Batch 500, Loss: 0.04158330928068608\n",
            "Epoch 35, Batch 600, Loss: 0.03440926604031119\n",
            "Epoch 35, Batch 700, Loss: 0.04236615807400085\n",
            "Epoch 35, Batch 800, Loss: 0.04794952495372854\n",
            "Epoch 36, Batch 100, Loss: 0.07106948572909459\n",
            "Epoch 36, Batch 200, Loss: 0.032338017540751024\n",
            "Epoch 36, Batch 300, Loss: 0.03438215943635441\n",
            "Epoch 36, Batch 400, Loss: 0.04260513715678826\n",
            "Epoch 36, Batch 500, Loss: 0.03583113689208403\n",
            "Epoch 36, Batch 600, Loss: 0.03350157391629182\n",
            "Epoch 36, Batch 700, Loss: 0.042341078335884956\n",
            "Epoch 36, Batch 800, Loss: 0.04065996975521557\n",
            "Epoch 37, Batch 100, Loss: 0.07295422261871863\n",
            "Epoch 37, Batch 200, Loss: 0.03935652319923975\n",
            "Epoch 37, Batch 300, Loss: 0.04694223277852871\n",
            "Epoch 37, Batch 400, Loss: 0.043649211649317295\n",
            "Epoch 37, Batch 500, Loss: 0.036653797754552214\n",
            "Epoch 37, Batch 600, Loss: 0.03933097940287553\n",
            "Epoch 37, Batch 700, Loss: 0.0380041643534787\n",
            "Epoch 37, Batch 800, Loss: 0.03316816909005865\n",
            "Epoch 38, Batch 100, Loss: 0.06368664821609854\n",
            "Epoch 38, Batch 200, Loss: 0.03505616442183964\n",
            "Epoch 38, Batch 300, Loss: 0.0383053606748581\n",
            "Epoch 38, Batch 400, Loss: 0.0420298790774541\n",
            "Epoch 38, Batch 500, Loss: 0.03418784342124127\n",
            "Epoch 38, Batch 600, Loss: 0.03660505305742845\n",
            "Epoch 38, Batch 700, Loss: 0.035707238549366596\n",
            "Epoch 38, Batch 800, Loss: 0.038905313766445036\n",
            "Epoch 39, Batch 100, Loss: 0.06520331669657026\n",
            "Epoch 39, Batch 200, Loss: 0.03642058639205061\n",
            "Epoch 39, Batch 300, Loss: 0.030323994273203425\n",
            "Epoch 39, Batch 400, Loss: 0.03356734041590244\n",
            "Epoch 39, Batch 500, Loss: 0.037338110021082685\n",
            "Epoch 39, Batch 600, Loss: 0.03646331227151677\n",
            "Epoch 39, Batch 700, Loss: 0.04124751523369923\n",
            "Epoch 39, Batch 800, Loss: 0.038549207350006325\n",
            "Epoch 40, Batch 100, Loss: 0.062117357259849085\n",
            "Epoch 40, Batch 200, Loss: 0.03572872755350545\n",
            "Epoch 40, Batch 300, Loss: 0.03566467250930146\n",
            "Epoch 40, Batch 400, Loss: 0.03952892297529616\n",
            "Epoch 40, Batch 500, Loss: 0.032041970762657\n",
            "Epoch 40, Batch 600, Loss: 0.038409290591953325\n",
            "Epoch 40, Batch 700, Loss: 0.03511199148371816\n",
            "Epoch 40, Batch 800, Loss: 0.035931112652178854\n",
            "Epoch 41, Batch 100, Loss: 0.07201264086645097\n",
            "Epoch 41, Batch 200, Loss: 0.04267460078466684\n",
            "Epoch 41, Batch 300, Loss: 0.03865291314141359\n",
            "Epoch 41, Batch 400, Loss: 0.03556021018885076\n",
            "Epoch 41, Batch 500, Loss: 0.033928147780243306\n",
            "Epoch 41, Batch 600, Loss: 0.03451398388075177\n",
            "Epoch 41, Batch 700, Loss: 0.03413420143187977\n",
            "Epoch 41, Batch 800, Loss: 0.034411457522655835\n",
            "Epoch 42, Batch 100, Loss: 0.058869382601696996\n",
            "Epoch 42, Batch 200, Loss: 0.03458153182233218\n",
            "Epoch 42, Batch 300, Loss: 0.04137822132441215\n",
            "Epoch 42, Batch 400, Loss: 0.033399603625293824\n",
            "Epoch 42, Batch 500, Loss: 0.03728616810403764\n",
            "Epoch 42, Batch 600, Loss: 0.031217393088154494\n",
            "Epoch 42, Batch 700, Loss: 0.040503916574234605\n",
            "Epoch 42, Batch 800, Loss: 0.03453876287559979\n",
            "Epoch 43, Batch 100, Loss: 0.06388734571519308\n",
            "Epoch 43, Batch 200, Loss: 0.039781402719672766\n",
            "Epoch 43, Batch 300, Loss: 0.03251444734865799\n",
            "Epoch 43, Batch 400, Loss: 0.029831814664648845\n",
            "Epoch 43, Batch 500, Loss: 0.03528586707310751\n",
            "Epoch 43, Batch 600, Loss: 0.031294910754077136\n",
            "Epoch 43, Batch 700, Loss: 0.03220501671778038\n",
            "Epoch 43, Batch 800, Loss: 0.03616025416878983\n",
            "Epoch 44, Batch 100, Loss: 0.061047058841213586\n",
            "Epoch 44, Batch 200, Loss: 0.03280076065449975\n",
            "Epoch 44, Batch 300, Loss: 0.03520376166445203\n",
            "Epoch 44, Batch 400, Loss: 0.033459409103379585\n",
            "Epoch 44, Batch 500, Loss: 0.03444802172016352\n",
            "Epoch 44, Batch 600, Loss: 0.03191288749221712\n",
            "Epoch 44, Batch 700, Loss: 0.03312796221638564\n",
            "Epoch 44, Batch 800, Loss: 0.03633307331649121\n",
            "Epoch 45, Batch 100, Loss: 0.0536914121452719\n",
            "Epoch 45, Batch 200, Loss: 0.028499538159230724\n",
            "Epoch 45, Batch 300, Loss: 0.04599776796065271\n",
            "Epoch 45, Batch 400, Loss: 0.031940888424869626\n",
            "Epoch 45, Batch 500, Loss: 0.03467041889904067\n",
            "Epoch 45, Batch 600, Loss: 0.03928789921570569\n",
            "Epoch 45, Batch 700, Loss: 0.03713130092364736\n",
            "Epoch 45, Batch 800, Loss: 0.035634189329575745\n",
            "Epoch 46, Batch 100, Loss: 0.05221728113712743\n",
            "Epoch 46, Batch 200, Loss: 0.03328066356421914\n",
            "Epoch 46, Batch 300, Loss: 0.03390698200208135\n",
            "Epoch 46, Batch 400, Loss: 0.036722520109033215\n",
            "Epoch 46, Batch 500, Loss: 0.030545309013687075\n",
            "Epoch 46, Batch 600, Loss: 0.0336868704110384\n",
            "Epoch 46, Batch 700, Loss: 0.03400097969686613\n",
            "Epoch 46, Batch 800, Loss: 0.039768556506605816\n",
            "Epoch 47, Batch 100, Loss: 0.053484869269886985\n",
            "Epoch 47, Batch 200, Loss: 0.038241229485720396\n",
            "Epoch 47, Batch 300, Loss: 0.03162507367436774\n",
            "Epoch 47, Batch 400, Loss: 0.035389274035696874\n",
            "Epoch 47, Batch 500, Loss: 0.03209246906102635\n",
            "Epoch 47, Batch 600, Loss: 0.024804758697282523\n",
            "Epoch 47, Batch 700, Loss: 0.04027352326666005\n",
            "Epoch 47, Batch 800, Loss: 0.026272865186911076\n",
            "Epoch 48, Batch 100, Loss: 0.05175576097448356\n",
            "Epoch 48, Batch 200, Loss: 0.02652084911824204\n",
            "Epoch 48, Batch 300, Loss: 0.030312024677405135\n",
            "Epoch 48, Batch 400, Loss: 0.03436589677876327\n",
            "Epoch 48, Batch 500, Loss: 0.03287641696864739\n",
            "Epoch 48, Batch 600, Loss: 0.028891273148474284\n",
            "Epoch 48, Batch 700, Loss: 0.027412321905139835\n",
            "Epoch 48, Batch 800, Loss: 0.0360944474634016\n",
            "Epoch 49, Batch 100, Loss: 0.05089841604611138\n",
            "Epoch 49, Batch 200, Loss: 0.03320356813026592\n",
            "Epoch 49, Batch 300, Loss: 0.02693494863458909\n",
            "Epoch 49, Batch 400, Loss: 0.03141421439126134\n",
            "Epoch 49, Batch 500, Loss: 0.02976625395240262\n",
            "Epoch 49, Batch 600, Loss: 0.03616790461121127\n",
            "Epoch 49, Batch 700, Loss: 0.030484770148759707\n",
            "Epoch 49, Batch 800, Loss: 0.031466858534840866\n",
            "Epoch 50, Batch 100, Loss: 0.04524864431004971\n",
            "Epoch 50, Batch 200, Loss: 0.01988472253549844\n",
            "Epoch 50, Batch 300, Loss: 0.03829668318619952\n",
            "Epoch 50, Batch 400, Loss: 0.026098519772058352\n",
            "Epoch 50, Batch 500, Loss: 0.040270347919431515\n",
            "Epoch 50, Batch 600, Loss: 0.027899583999533205\n",
            "Epoch 50, Batch 700, Loss: 0.03434191479231231\n",
            "Epoch 50, Batch 800, Loss: 0.0312522823153995\n",
            "Epoch 51, Batch 100, Loss: 0.05698426150600426\n",
            "Epoch 51, Batch 200, Loss: 0.034855134607059884\n",
            "Epoch 51, Batch 300, Loss: 0.028288968182168903\n",
            "Epoch 51, Batch 400, Loss: 0.025148454974987543\n",
            "Epoch 51, Batch 500, Loss: 0.031467693964368666\n",
            "Epoch 51, Batch 600, Loss: 0.02303681704623159\n",
            "Epoch 51, Batch 700, Loss: 0.025840057567111215\n",
            "Epoch 51, Batch 800, Loss: 0.02932036475278437\n",
            "Epoch 52, Batch 100, Loss: 0.056692896945751275\n",
            "Epoch 52, Batch 200, Loss: 0.02853296698653139\n",
            "Epoch 52, Batch 300, Loss: 0.028432684609433635\n",
            "Epoch 52, Batch 400, Loss: 0.0308270791528048\n",
            "Epoch 52, Batch 500, Loss: 0.02760390404961072\n",
            "Epoch 52, Batch 600, Loss: 0.029708953778026626\n",
            "Epoch 52, Batch 700, Loss: 0.03211836617207155\n",
            "Epoch 52, Batch 800, Loss: 0.034569982271350455\n",
            "Epoch 53, Batch 100, Loss: 0.051867285047192124\n",
            "Epoch 53, Batch 200, Loss: 0.02901898128533503\n",
            "Epoch 53, Batch 300, Loss: 0.030105238269607072\n",
            "Epoch 53, Batch 400, Loss: 0.02595772375469096\n",
            "Epoch 53, Batch 500, Loss: 0.03408883100491948\n",
            "Epoch 53, Batch 600, Loss: 0.03029206760285888\n",
            "Epoch 53, Batch 700, Loss: 0.030461577175883575\n",
            "Epoch 53, Batch 800, Loss: 0.026664779583516065\n",
            "Epoch 54, Batch 100, Loss: 0.052944318521767855\n",
            "Epoch 54, Batch 200, Loss: 0.03018607937905472\n",
            "Epoch 54, Batch 300, Loss: 0.03162954426486977\n",
            "Epoch 54, Batch 400, Loss: 0.027763052230002357\n",
            "Epoch 54, Batch 500, Loss: 0.028957223033648914\n",
            "Epoch 54, Batch 600, Loss: 0.02503539025434293\n",
            "Epoch 54, Batch 700, Loss: 0.02681296224502148\n",
            "Epoch 54, Batch 800, Loss: 0.035616234291810545\n",
            "Epoch 55, Batch 100, Loss: 0.05019246282870881\n",
            "Epoch 55, Batch 200, Loss: 0.028866555294953286\n",
            "Epoch 55, Batch 300, Loss: 0.02173346627911087\n",
            "Epoch 55, Batch 400, Loss: 0.029728063945658505\n",
            "Epoch 55, Batch 500, Loss: 0.027025088053778745\n",
            "Epoch 55, Batch 600, Loss: 0.026785171680385248\n",
            "Epoch 55, Batch 700, Loss: 0.029470030553638935\n",
            "Epoch 55, Batch 800, Loss: 0.02973575797484955\n",
            "Epoch 56, Batch 100, Loss: 0.0575046545045916\n",
            "Epoch 56, Batch 200, Loss: 0.02639477885386441\n",
            "Epoch 56, Batch 300, Loss: 0.030341125116101465\n",
            "Epoch 56, Batch 400, Loss: 0.028165749096078798\n",
            "Epoch 56, Batch 500, Loss: 0.03459996958379634\n",
            "Epoch 56, Batch 600, Loss: 0.028286186663317493\n",
            "Epoch 56, Batch 700, Loss: 0.02801327315683011\n",
            "Epoch 56, Batch 800, Loss: 0.024807282999390737\n",
            "Epoch 57, Batch 100, Loss: 0.05153172420803458\n",
            "Epoch 57, Batch 200, Loss: 0.03163381858146749\n",
            "Epoch 57, Batch 300, Loss: 0.025883219121024013\n",
            "Epoch 57, Batch 400, Loss: 0.029491316615603863\n",
            "Epoch 57, Batch 500, Loss: 0.0294186942523811\n",
            "Epoch 57, Batch 600, Loss: 0.026743961741449312\n",
            "Epoch 57, Batch 700, Loss: 0.02843021651613526\n",
            "Epoch 57, Batch 800, Loss: 0.02464239744702354\n",
            "Epoch 58, Batch 100, Loss: 0.04822912142932182\n",
            "Epoch 58, Batch 200, Loss: 0.023924301097285935\n",
            "Epoch 58, Batch 300, Loss: 0.025211319078225644\n",
            "Epoch 58, Batch 400, Loss: 0.02871593371150084\n",
            "Epoch 58, Batch 500, Loss: 0.028486665826058015\n",
            "Epoch 58, Batch 600, Loss: 0.03157864532317035\n",
            "Epoch 58, Batch 700, Loss: 0.02347200552059803\n",
            "Epoch 58, Batch 800, Loss: 0.026610546497395263\n",
            "Epoch 59, Batch 100, Loss: 0.05464656894269865\n",
            "Epoch 59, Batch 200, Loss: 0.025274028088897466\n",
            "Epoch 59, Batch 300, Loss: 0.028879385375475977\n",
            "Epoch 59, Batch 400, Loss: 0.03133439215598628\n",
            "Epoch 59, Batch 500, Loss: 0.020344097274501108\n",
            "Epoch 59, Batch 600, Loss: 0.025071922825445653\n",
            "Epoch 59, Batch 700, Loss: 0.027016058552835603\n",
            "Epoch 59, Batch 800, Loss: 0.03208738464338239\n",
            "Epoch 60, Batch 100, Loss: 0.04440572550534853\n",
            "Epoch 60, Batch 200, Loss: 0.030245982960914262\n",
            "Epoch 60, Batch 300, Loss: 0.03325206718640402\n",
            "Epoch 60, Batch 400, Loss: 0.02542763566307258\n",
            "Epoch 60, Batch 500, Loss: 0.027497647808049804\n",
            "Epoch 60, Batch 600, Loss: 0.03323887554521207\n",
            "Epoch 60, Batch 700, Loss: 0.02483156592439627\n",
            "Epoch 60, Batch 800, Loss: 0.032394496857305055\n",
            "Epoch 61, Batch 100, Loss: 0.05235714284528512\n",
            "Epoch 61, Batch 200, Loss: 0.032243523895740506\n",
            "Epoch 61, Batch 300, Loss: 0.02854634429924772\n",
            "Epoch 61, Batch 400, Loss: 0.023529580341564726\n",
            "Epoch 61, Batch 500, Loss: 0.02603552266547922\n",
            "Epoch 61, Batch 600, Loss: 0.02793815135024488\n",
            "Epoch 61, Batch 700, Loss: 0.030346358303213493\n",
            "Epoch 61, Batch 800, Loss: 0.024316073170048184\n",
            "Epoch 62, Batch 100, Loss: 0.04511975049797911\n",
            "Epoch 62, Batch 200, Loss: 0.032291204242501405\n",
            "Epoch 62, Batch 300, Loss: 0.025910811789217406\n",
            "Epoch 62, Batch 400, Loss: 0.02637766950967489\n",
            "Epoch 62, Batch 500, Loss: 0.024218053827353286\n",
            "Epoch 62, Batch 600, Loss: 0.02342798304394819\n",
            "Epoch 62, Batch 700, Loss: 0.025700465639820324\n",
            "Epoch 62, Batch 800, Loss: 0.02900988144101575\n",
            "Epoch 63, Batch 100, Loss: 0.04299438068555901\n",
            "Epoch 63, Batch 200, Loss: 0.02477277315192623\n",
            "Epoch 63, Batch 300, Loss: 0.022740952801541425\n",
            "Epoch 63, Batch 400, Loss: 0.027220920253312214\n",
            "Epoch 63, Batch 500, Loss: 0.03352958073199261\n",
            "Epoch 63, Batch 600, Loss: 0.024977330665278713\n",
            "Epoch 63, Batch 700, Loss: 0.022491751119378022\n",
            "Epoch 63, Batch 800, Loss: 0.03561752603534842\n",
            "Epoch 64, Batch 100, Loss: 0.04250952716742176\n",
            "Epoch 64, Batch 200, Loss: 0.02226687082351418\n",
            "Epoch 64, Batch 300, Loss: 0.02383447807689663\n",
            "Epoch 64, Batch 400, Loss: 0.023450720738619566\n",
            "Epoch 64, Batch 500, Loss: 0.028441160523798317\n",
            "Epoch 64, Batch 600, Loss: 0.02523260145186214\n",
            "Epoch 64, Batch 700, Loss: 0.030338064911484254\n",
            "Epoch 64, Batch 800, Loss: 0.020396100173820743\n",
            "Epoch 65, Batch 100, Loss: 0.044468217942630874\n",
            "Epoch 65, Batch 200, Loss: 0.0227982344746124\n",
            "Epoch 65, Batch 300, Loss: 0.030308295141439887\n",
            "Epoch 65, Batch 400, Loss: 0.029889793434995225\n",
            "Epoch 65, Batch 500, Loss: 0.022919577257707716\n",
            "Epoch 65, Batch 600, Loss: 0.02470253329782281\n",
            "Epoch 65, Batch 700, Loss: 0.028540024995454587\n",
            "Epoch 65, Batch 800, Loss: 0.02446358027460519\n",
            "Epoch 66, Batch 100, Loss: 0.04391345713462215\n",
            "Epoch 66, Batch 200, Loss: 0.0264988767815521\n",
            "Epoch 66, Batch 300, Loss: 0.022905524589004925\n",
            "Epoch 66, Batch 400, Loss: 0.026794578548287974\n",
            "Epoch 66, Batch 500, Loss: 0.024047336246003397\n",
            "Epoch 66, Batch 600, Loss: 0.027863577803364025\n",
            "Epoch 66, Batch 700, Loss: 0.023752691309637156\n",
            "Epoch 66, Batch 800, Loss: 0.023578905385074903\n",
            "Epoch 67, Batch 100, Loss: 0.042745859217247925\n",
            "Epoch 67, Batch 200, Loss: 0.028314057140378282\n",
            "Epoch 67, Batch 300, Loss: 0.02290020756656304\n",
            "Epoch 67, Batch 400, Loss: 0.027115623900899662\n",
            "Epoch 67, Batch 500, Loss: 0.021189509853720666\n",
            "Epoch 67, Batch 600, Loss: 0.029857947732089087\n",
            "Epoch 67, Batch 700, Loss: 0.027921594047220423\n",
            "Epoch 67, Batch 800, Loss: 0.023198584840865805\n",
            "Epoch 68, Batch 100, Loss: 0.04776539271872025\n",
            "Epoch 68, Batch 200, Loss: 0.024480569422594273\n",
            "Epoch 68, Batch 300, Loss: 0.022096952405117918\n",
            "Epoch 68, Batch 400, Loss: 0.022361028566374442\n",
            "Epoch 68, Batch 500, Loss: 0.02660029315680731\n",
            "Epoch 68, Batch 600, Loss: 0.024274055812566076\n",
            "Epoch 68, Batch 700, Loss: 0.02351331688842038\n",
            "Epoch 68, Batch 800, Loss: 0.0289269353315467\n",
            "Epoch 69, Batch 100, Loss: 0.043271451001928654\n",
            "Epoch 69, Batch 200, Loss: 0.025137055566883646\n",
            "Epoch 69, Batch 300, Loss: 0.024135858477093278\n",
            "Epoch 69, Batch 400, Loss: 0.022755438256426716\n",
            "Epoch 69, Batch 500, Loss: 0.023740590950474143\n",
            "Epoch 69, Batch 600, Loss: 0.02602782605565153\n",
            "Epoch 69, Batch 700, Loss: 0.02704369619372301\n",
            "Epoch 69, Batch 800, Loss: 0.022478735666372814\n",
            "Epoch 70, Batch 100, Loss: 0.03570168597914744\n",
            "Epoch 70, Batch 200, Loss: 0.02252459745737724\n",
            "Epoch 70, Batch 300, Loss: 0.027549670953303577\n",
            "Epoch 70, Batch 400, Loss: 0.02731930852867663\n",
            "Epoch 70, Batch 500, Loss: 0.02462417632807046\n",
            "Epoch 70, Batch 600, Loss: 0.02296870242105797\n",
            "Epoch 70, Batch 700, Loss: 0.019931237219716423\n",
            "Epoch 70, Batch 800, Loss: 0.022876609396189452\n",
            "Epoch 71, Batch 100, Loss: 0.04039406620140653\n",
            "Epoch 71, Batch 200, Loss: 0.021117405674303882\n",
            "Epoch 71, Batch 300, Loss: 0.027436383023741656\n",
            "Epoch 71, Batch 400, Loss: 0.026676216820051196\n",
            "Epoch 71, Batch 500, Loss: 0.024619069492910057\n",
            "Epoch 71, Batch 600, Loss: 0.02304974399274215\n",
            "Epoch 71, Batch 700, Loss: 0.0277912835724419\n",
            "Epoch 71, Batch 800, Loss: 0.025130771269905382\n",
            "Epoch 72, Batch 100, Loss: 0.03936078076309059\n",
            "Epoch 72, Batch 200, Loss: 0.02130168953095563\n",
            "Epoch 72, Batch 300, Loss: 0.02328595154103823\n",
            "Epoch 72, Batch 400, Loss: 0.022996941260935273\n",
            "Epoch 72, Batch 500, Loss: 0.02173714228440076\n",
            "Epoch 72, Batch 600, Loss: 0.02691491653182311\n",
            "Epoch 72, Batch 700, Loss: 0.01920065382408211\n",
            "Epoch 72, Batch 800, Loss: 0.018737050966883544\n",
            "Epoch 73, Batch 100, Loss: 0.0443397638812894\n",
            "Epoch 73, Batch 200, Loss: 0.018716634568409062\n",
            "Epoch 73, Batch 300, Loss: 0.02209511945489794\n",
            "Epoch 73, Batch 400, Loss: 0.02308726927265525\n",
            "Epoch 73, Batch 500, Loss: 0.02343953040894121\n",
            "Epoch 73, Batch 600, Loss: 0.023821598643553443\n",
            "Epoch 73, Batch 700, Loss: 0.020885172100097407\n",
            "Epoch 73, Batch 800, Loss: 0.03162056140892673\n",
            "Epoch 74, Batch 100, Loss: 0.03877026069560088\n",
            "Epoch 74, Batch 200, Loss: 0.021014594392981964\n",
            "Epoch 74, Batch 300, Loss: 0.03152615073951893\n",
            "Epoch 74, Batch 400, Loss: 0.0200314198070555\n",
            "Epoch 74, Batch 500, Loss: 0.02016369954071706\n",
            "Epoch 74, Batch 600, Loss: 0.02603100993350381\n",
            "Epoch 74, Batch 700, Loss: 0.02749278621864505\n",
            "Epoch 74, Batch 800, Loss: 0.019610650159738726\n",
            "Epoch 75, Batch 100, Loss: 0.04302129319839878\n",
            "Epoch 75, Batch 200, Loss: 0.02013035707990639\n",
            "Epoch 75, Batch 300, Loss: 0.02317967078764923\n",
            "Epoch 75, Batch 400, Loss: 0.02568353012437001\n",
            "Epoch 75, Batch 500, Loss: 0.020429232187598245\n",
            "Epoch 75, Batch 600, Loss: 0.020422184468479827\n",
            "Epoch 75, Batch 700, Loss: 0.018380482307693454\n",
            "Epoch 75, Batch 800, Loss: 0.02324206925404724\n",
            "Epoch 76, Batch 100, Loss: 0.04145094620442251\n",
            "Epoch 76, Batch 200, Loss: 0.02282489370845724\n",
            "Epoch 76, Batch 300, Loss: 0.0179858291358687\n",
            "Epoch 76, Batch 400, Loss: 0.02241355882084463\n",
            "Epoch 76, Batch 500, Loss: 0.022814049331645946\n",
            "Epoch 76, Batch 600, Loss: 0.027815514635585716\n",
            "Epoch 76, Batch 700, Loss: 0.01929155304969754\n",
            "Epoch 76, Batch 800, Loss: 0.029029191297013313\n",
            "Epoch 77, Batch 100, Loss: 0.042008696764387427\n",
            "Epoch 77, Batch 200, Loss: 0.023752266057999804\n",
            "Epoch 77, Batch 300, Loss: 0.02610747854996589\n",
            "Epoch 77, Batch 400, Loss: 0.02179180299775908\n",
            "Epoch 77, Batch 500, Loss: 0.02456104733311804\n",
            "Epoch 77, Batch 600, Loss: 0.0218963702696783\n",
            "Epoch 77, Batch 700, Loss: 0.020562412830913673\n",
            "Epoch 77, Batch 800, Loss: 0.02049571863928577\n",
            "Epoch 78, Batch 100, Loss: 0.03941178421140648\n",
            "Epoch 78, Batch 200, Loss: 0.024983514376799576\n",
            "Epoch 78, Batch 300, Loss: 0.020540574673213997\n",
            "Epoch 78, Batch 400, Loss: 0.019299330112407916\n",
            "Epoch 78, Batch 500, Loss: 0.02157898499935982\n",
            "Epoch 78, Batch 600, Loss: 0.018994782742811366\n",
            "Epoch 78, Batch 700, Loss: 0.024711394937476143\n",
            "Epoch 78, Batch 800, Loss: 0.022406678266706878\n",
            "Epoch 79, Batch 100, Loss: 0.030398125909196095\n",
            "Epoch 79, Batch 200, Loss: 0.023464897231606302\n",
            "Epoch 79, Batch 300, Loss: 0.020835553753422574\n",
            "Epoch 79, Batch 400, Loss: 0.01720792908628937\n",
            "Epoch 79, Batch 500, Loss: 0.022129973165865523\n",
            "Epoch 79, Batch 600, Loss: 0.02613861385092605\n",
            "Epoch 79, Batch 700, Loss: 0.025170206000912004\n",
            "Epoch 79, Batch 800, Loss: 0.02040770613064524\n",
            "Epoch 80, Batch 100, Loss: 0.03656956115708453\n",
            "Epoch 80, Batch 200, Loss: 0.01970915318262996\n",
            "Epoch 80, Batch 300, Loss: 0.02183907820872264\n",
            "Epoch 80, Batch 400, Loss: 0.02395395210769493\n",
            "Epoch 80, Batch 500, Loss: 0.032139620979432945\n",
            "Epoch 80, Batch 600, Loss: 0.01999953326478135\n",
            "Epoch 80, Batch 700, Loss: 0.0223713014065288\n",
            "Epoch 80, Batch 800, Loss: 0.02314173910912359\n",
            "Epoch 81, Batch 100, Loss: 0.03673503148369491\n",
            "Epoch 81, Batch 200, Loss: 0.016281295575900002\n",
            "Epoch 81, Batch 300, Loss: 0.02179500118087162\n",
            "Epoch 81, Batch 400, Loss: 0.019705253449792508\n",
            "Epoch 81, Batch 500, Loss: 0.021067904708324932\n",
            "Epoch 81, Batch 600, Loss: 0.026135065001435576\n",
            "Epoch 81, Batch 700, Loss: 0.019343449903535658\n",
            "Epoch 81, Batch 800, Loss: 0.019178767846606205\n",
            "Epoch 82, Batch 100, Loss: 0.03555491548439022\n",
            "Epoch 82, Batch 200, Loss: 0.02212620260368567\n",
            "Epoch 82, Batch 300, Loss: 0.020317424485692754\n",
            "Epoch 82, Batch 400, Loss: 0.021868317135085816\n",
            "Epoch 82, Batch 500, Loss: 0.018220987073436844\n",
            "Epoch 82, Batch 600, Loss: 0.018359083528484917\n",
            "Epoch 82, Batch 700, Loss: 0.017584036481857766\n",
            "Epoch 82, Batch 800, Loss: 0.022641374371596613\n",
            "Epoch 83, Batch 100, Loss: 0.04128229659429053\n",
            "Epoch 83, Batch 200, Loss: 0.020352494090911933\n",
            "Epoch 83, Batch 300, Loss: 0.027616212896537037\n",
            "Epoch 83, Batch 400, Loss: 0.020731609388894866\n",
            "Epoch 83, Batch 500, Loss: 0.019871287762071005\n",
            "Epoch 83, Batch 600, Loss: 0.024942394042736852\n",
            "Epoch 83, Batch 700, Loss: 0.029412076122243887\n",
            "Epoch 83, Batch 800, Loss: 0.022686608880758286\n",
            "Epoch 84, Batch 100, Loss: 0.0359827231609961\n",
            "Epoch 84, Batch 200, Loss: 0.023631856242427602\n",
            "Epoch 84, Batch 300, Loss: 0.020004932877782267\n",
            "Epoch 84, Batch 400, Loss: 0.01592293468886055\n",
            "Epoch 84, Batch 500, Loss: 0.017476258253591368\n",
            "Epoch 84, Batch 600, Loss: 0.018905797449187956\n",
            "Epoch 84, Batch 700, Loss: 0.020288644338652375\n",
            "Epoch 84, Batch 800, Loss: 0.020057138976990244\n",
            "Epoch 85, Batch 100, Loss: 0.03705982630592189\n",
            "Epoch 85, Batch 200, Loss: 0.019731410684180446\n",
            "Epoch 85, Batch 300, Loss: 0.01699373217241373\n",
            "Epoch 85, Batch 400, Loss: 0.019298225453239864\n",
            "Epoch 85, Batch 500, Loss: 0.02260472681140527\n",
            "Epoch 85, Batch 600, Loss: 0.02064654401852749\n",
            "Epoch 85, Batch 700, Loss: 0.023129890890559182\n",
            "Epoch 85, Batch 800, Loss: 0.017645914390304825\n",
            "Epoch 86, Batch 100, Loss: 0.04004748386447318\n",
            "Epoch 86, Batch 200, Loss: 0.01747940363798989\n",
            "Epoch 86, Batch 300, Loss: 0.01873951911140466\n",
            "Epoch 86, Batch 400, Loss: 0.019787148689938475\n",
            "Epoch 86, Batch 500, Loss: 0.016331870340072784\n",
            "Epoch 86, Batch 600, Loss: 0.017912031781161204\n",
            "Epoch 86, Batch 700, Loss: 0.02013688230304979\n",
            "Epoch 86, Batch 800, Loss: 0.025142661137506364\n",
            "Epoch 87, Batch 100, Loss: 0.033378992673242465\n",
            "Epoch 87, Batch 200, Loss: 0.01581187000818318\n",
            "Epoch 87, Batch 300, Loss: 0.01895573869915097\n",
            "Epoch 87, Batch 400, Loss: 0.01583532541641034\n",
            "Epoch 87, Batch 500, Loss: 0.020854529659554828\n",
            "Epoch 87, Batch 600, Loss: 0.016122762449085712\n",
            "Epoch 87, Batch 700, Loss: 0.016014416242542212\n",
            "Epoch 87, Batch 800, Loss: 0.021495740532118363\n",
            "Epoch 88, Batch 100, Loss: 0.03146164240926737\n",
            "Epoch 88, Batch 200, Loss: 0.017651935019530357\n",
            "Epoch 88, Batch 300, Loss: 0.018445378090546\n",
            "Epoch 88, Batch 400, Loss: 0.01605224433063995\n",
            "Epoch 88, Batch 500, Loss: 0.023565177757991478\n",
            "Epoch 88, Batch 600, Loss: 0.018924195002764464\n",
            "Epoch 88, Batch 700, Loss: 0.018655581456696383\n",
            "Epoch 88, Batch 800, Loss: 0.020144808133190962\n",
            "Epoch 89, Batch 100, Loss: 0.030253015084163053\n",
            "Epoch 89, Batch 200, Loss: 0.01574702491372591\n",
            "Epoch 89, Batch 300, Loss: 0.02168561219856201\n",
            "Epoch 89, Batch 400, Loss: 0.019156315166619607\n",
            "Epoch 89, Batch 500, Loss: 0.016504240341600963\n",
            "Epoch 89, Batch 600, Loss: 0.025316298160760198\n",
            "Epoch 89, Batch 700, Loss: 0.02348456109437393\n",
            "Epoch 89, Batch 800, Loss: 0.020986941230949015\n",
            "Epoch 90, Batch 100, Loss: 0.04104051143454854\n",
            "Epoch 90, Batch 200, Loss: 0.021677140487445287\n",
            "Epoch 90, Batch 300, Loss: 0.01777227957107243\n",
            "Epoch 90, Batch 400, Loss: 0.021730646472860826\n",
            "Epoch 90, Batch 500, Loss: 0.018817789847380483\n",
            "Epoch 90, Batch 600, Loss: 0.019382096272020134\n",
            "Epoch 90, Batch 700, Loss: 0.018110978730255738\n",
            "Epoch 90, Batch 800, Loss: 0.017599256503017385\n",
            "Epoch 91, Batch 100, Loss: 0.033259818180667935\n",
            "Epoch 91, Batch 200, Loss: 0.01869610175024718\n",
            "Epoch 91, Batch 300, Loss: 0.016130873096772122\n",
            "Epoch 91, Batch 400, Loss: 0.018346704727155157\n",
            "Epoch 91, Batch 500, Loss: 0.01620097413484473\n",
            "Epoch 91, Batch 600, Loss: 0.018580865603871642\n",
            "Epoch 91, Batch 700, Loss: 0.017898941058811034\n",
            "Epoch 91, Batch 800, Loss: 0.017625054650998208\n",
            "Epoch 92, Batch 100, Loss: 0.03805150936314021\n",
            "Epoch 92, Batch 200, Loss: 0.01927954116850742\n",
            "Epoch 92, Batch 300, Loss: 0.017034915371332317\n",
            "Epoch 92, Batch 400, Loss: 0.019395517127413768\n",
            "Epoch 92, Batch 500, Loss: 0.019952685089956502\n",
            "Epoch 92, Batch 600, Loss: 0.017585692853608635\n",
            "Epoch 92, Batch 700, Loss: 0.017398766917176546\n",
            "Epoch 92, Batch 800, Loss: 0.020372032352606765\n",
            "Epoch 93, Batch 100, Loss: 0.03857916827284498\n",
            "Epoch 93, Batch 200, Loss: 0.02288521616384969\n",
            "Epoch 93, Batch 300, Loss: 0.016861180682026314\n",
            "Epoch 93, Batch 400, Loss: 0.022709123296081088\n",
            "Epoch 93, Batch 500, Loss: 0.015673326604010072\n",
            "Epoch 93, Batch 600, Loss: 0.014927788052882534\n",
            "Epoch 93, Batch 700, Loss: 0.019462899402715266\n",
            "Epoch 93, Batch 800, Loss: 0.02378907834063284\n",
            "Epoch 94, Batch 100, Loss: 0.02715438670838921\n",
            "Epoch 94, Batch 200, Loss: 0.017176727228506935\n",
            "Epoch 94, Batch 300, Loss: 0.018876035585271893\n",
            "Epoch 94, Batch 400, Loss: 0.01756881017681735\n",
            "Epoch 94, Batch 500, Loss: 0.01682030986630707\n",
            "Epoch 94, Batch 600, Loss: 0.015212118244089651\n",
            "Epoch 94, Batch 700, Loss: 0.018149261974031106\n",
            "Epoch 94, Batch 800, Loss: 0.019888729375379624\n",
            "Epoch 95, Batch 100, Loss: 0.03345987091801362\n",
            "Epoch 95, Batch 200, Loss: 0.01982278423514799\n",
            "Epoch 95, Batch 300, Loss: 0.013204135460255203\n",
            "Epoch 95, Batch 400, Loss: 0.021201095120050015\n",
            "Epoch 95, Batch 500, Loss: 0.013115050935011823\n",
            "Epoch 95, Batch 600, Loss: 0.018724653914250666\n",
            "Epoch 95, Batch 700, Loss: 0.01948811266993289\n",
            "Epoch 95, Batch 800, Loss: 0.01531465299663978\n",
            "Epoch 96, Batch 100, Loss: 0.03664971288861125\n",
            "Epoch 96, Batch 200, Loss: 0.016852595308737364\n",
            "Epoch 96, Batch 300, Loss: 0.019367596361844333\n",
            "Epoch 96, Batch 400, Loss: 0.02003472601165413\n",
            "Epoch 96, Batch 500, Loss: 0.016904666398768312\n",
            "Epoch 96, Batch 600, Loss: 0.01772918003174709\n",
            "Epoch 96, Batch 700, Loss: 0.021059785401448608\n",
            "Epoch 96, Batch 800, Loss: 0.01914349546583253\n",
            "Epoch 97, Batch 100, Loss: 0.029982679981330876\n",
            "Epoch 97, Batch 200, Loss: 0.022224772047484295\n",
            "Epoch 97, Batch 300, Loss: 0.02175386619517667\n",
            "Epoch 97, Batch 400, Loss: 0.019411742646188942\n",
            "Epoch 97, Batch 500, Loss: 0.014815638662694254\n",
            "Epoch 97, Batch 600, Loss: 0.02120598173176404\n",
            "Epoch 97, Batch 700, Loss: 0.020969534984906203\n",
            "Epoch 97, Batch 800, Loss: 0.015182531000173185\n",
            "Epoch 98, Batch 100, Loss: 0.02892617601355596\n",
            "Epoch 98, Batch 200, Loss: 0.02200408215248899\n",
            "Epoch 98, Batch 300, Loss: 0.015319877588772214\n",
            "Epoch 98, Batch 400, Loss: 0.022282809395037476\n",
            "Epoch 98, Batch 500, Loss: 0.014790991935005876\n",
            "Epoch 98, Batch 600, Loss: 0.018056349173130003\n",
            "Epoch 98, Batch 700, Loss: 0.018011010899790562\n",
            "Epoch 98, Batch 800, Loss: 0.01672139453396085\n",
            "Epoch 99, Batch 100, Loss: 0.02713342085327895\n",
            "Epoch 99, Batch 200, Loss: 0.019754259988840204\n",
            "Epoch 99, Batch 300, Loss: 0.016529592095685074\n",
            "Epoch 99, Batch 400, Loss: 0.01765693872999691\n",
            "Epoch 99, Batch 500, Loss: 0.01927715737678227\n",
            "Epoch 99, Batch 600, Loss: 0.019574137451127172\n",
            "Epoch 99, Batch 700, Loss: 0.02173283172542142\n",
            "Epoch 99, Batch 800, Loss: 0.015275003048445797\n",
            "Epoch 100, Batch 100, Loss: 0.035081219575658906\n",
            "Epoch 100, Batch 200, Loss: 0.018903337428928354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-16-bf4e475b150c>\", line 21, in <cell line: 4>\n",
            "    running_loss += loss.item()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-bf4e475b150c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbblqsxdvwX2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}